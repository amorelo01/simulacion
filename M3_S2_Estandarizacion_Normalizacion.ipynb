{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amorelo01/simulacion/blob/main/M3_S2_Estandarizacion_Normalizacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p><img alt=\"banner\" height=\"252px\" width=\"1080px\" src=\"https://docs.google.com/uc?export=download&id=18D9zTLyHjMFbwtI2Eenr0l5oGeH9a1Wq\"  align=\"center\" hspace=\"10px\" vspace=\"0px\" ></p>"
      ],
      "metadata": {
        "id": "8VV-q-Zhxu1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Introducción** </font>\n",
        "\n",
        "El escalado de características (*feature scaling*) es una técnica de preprocesamiento que transforma los valores de las características a una escala similar, asegurando que todas las características contribuyan de manera equitativa al modelo. Es esencial para conjuntos de datos con características de rangos, unidades o magnitudes variables.\n",
        "\n",
        "El escalado de características ofrece varios beneficios, especialmente al trabajar con modelos de aprendizaje automático:\n",
        "\n",
        "* **Mejora el rendimiento del modelo**: Muchos modelos, especialmente los que se basan en cálculos de distancia, dependen de la escala de las características. El escalado asegura que cada característica contribuya de manera equitativa, evitando que las características con valores más grandes dominen.\n",
        "\n",
        "* **Convergencia más rápida**: Algoritmos como el *gradient descent*  funcionan mejor y convergen más rápido cuando las características están en la misma escala\n",
        "\n",
        "* **Previene el sesgo**: Sin escalado de características, las características con magnitudes más grandes podrían influir desproporcionadamente en el modelo, lo que llevaría a resultados sesgados.\n",
        "\n",
        "* **Mejor interpretabilidad**: Escalar las características facilita la interpretación de la importancia de cada una, especialmente en modelos que usan coeficientes (como la regresión lineal o logística), ya que los coeficientes no estarán dominados por la escala de las características.\n",
        "\n",
        "* **Consistencia entre características**: Para modelos que combinan diferentes tipos de características (por ejemplo, numéricas y categóricas), el escalado asegura que el modelo no favorezca características solo porque tienen diferentes unidades o rangos.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T2KY4Xc5kWqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='8EC044'> **¿Qué impacto tiene el escalar los datos?** </font>"
      ],
      "metadata": {
        "id": "VsS5Eo3JaEZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere el siguiente ejemplo, en el que en un programa de riesgo cardiovascular deseamos agrupar un conjunto de personas de acuerdo a su similaridad en peso y estatura.\n",
        "\n",
        "Hemos usado el  algoritmo de agrupación `k-means` (el cual discutiremos en el siguiente curso de la linea) para llevar a cabo esta tarea. En el hemos usado los datos sin escalar y despues de escalar: **¿Qué diferencia observa?**"
      ],
      "metadata": {
        "id": "1hOYqFhhX0mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "data = {\n",
        "    'Estatura (m)': [1.69, 1.98, 1.87, 1.80, 1.58, 1.58, 1.53, 1.93, 1.80, 1.85,\n",
        "           1.51, 1.99, 1.92, 1.61, 1.59, 1.59, 1.65, 1.76, 1.72, 1.65],\n",
        "    'Peso (kg)': [80.59, 76.97, 68.61, 68.32, 72.80, 89.26, 59.98, 85.71, 79.62, 62.32,\n",
        "                    80.38, 98.53, 83.25, 77.44, 98.28, 60.42, 65.23, 64.88, 84.21, 72.01]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Fit the model to the unscaled data\n",
        "kmeans = KMeans(n_clusters=3, n_init= 10,random_state=42)\n",
        "kmeans.fit(df)\n",
        "labels = kmeans.labels_\n",
        "df['Cluster'] = labels\n",
        "\n",
        "# Fit the model  with scaled data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_scaled[['Estatura (m)', 'Peso (kg)']])\n",
        "kmeans = KMeans(n_clusters=3, n_init= 10, random_state=42)\n",
        "kmeans.fit(scaled_data)\n",
        "labels = kmeans.labels_\n",
        "df_scaled['Cluster'] = labels\n",
        "\n",
        "\n",
        "# Create subplots\n",
        "fig = make_subplots(rows=1, cols=2, subplot_titles=['Sin escalar', 'Escalados'])\n",
        "\n",
        "# Add traces to subplots\n",
        "fig.add_trace(go.Scatter(x=df['Estatura (m)'], y=df['Peso (kg)'],\n",
        "                         mode='markers', marker_color=df['Cluster'], marker_size=10, showlegend=False), row=1, col=1)\n",
        "fig.add_trace(go.Scatter(x=df_scaled['Estatura (m)'], y=df_scaled['Peso (kg)'],\n",
        "                         mode='markers', marker_color=df_scaled['Cluster'], marker_size=10, showlegend=False), row=1, col=2)\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(title='Estatura (m) vs Peso (kg)', xaxis1_title='Estatura (m)', xaxis2_title='Estatura (m)',\n",
        "                  yaxis1_title='Peso (kg)', yaxis2_title='Peso (kg)')\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UQ6VmXFDQFiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color='8EC044'> **Rescalar, normalizar, estandarizar** </font>"
      ],
      "metadata": {
        "id": "qWhVDPA0aKq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El término escalamiento, normalización y estandarización se usan frecuentemente en ciencia de datos. Algunas veces se usan de forma  intercambiable. sin embargo, en aras de estructurar la discusión vamos a definir brevemente cada uno de ellos:\n",
        "\n",
        "* <font color='46B8A9'> **Reescalar** </font> un vector significa sumar o restar una constante y luego multiplicar o dividir por una constante, como lo harías para cambiar las unidades de medida de los datos, por ejemplo, para convertir una temperatura de Celsius a Fahrenheit.\n",
        "\n",
        "* <font color='46B8A9'> **Normalizar** </font>  un vector, con mayor frecuencia, significa dividir por una norma del vector, por ejemplo,reescalar por el mínimo y el rango del vector, para hacer que todos los elementos queden entre 0 y 1.\n",
        "\n",
        "* <font color='46B8A9'> **Estandarizar** </font>  un vector, con mayor frecuencia, significa restar una medida de localización y dividir por una medida de escala. Por ejemplo, si el vector contiene valores aleatorios con una distribución gaussiana, podrías restar la media y dividir por la desviación estándar, obteniendo así una variable aleatoria \"normal estándar\" con media 0 y desviación estándar 1."
      ],
      "metadata": {
        "id": "tUocy44-CbQ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunas de las técnicas más comunes se resumen en la siguiente tabla."
      ],
      "metadata": {
        "id": "NmJu-YjsYoDe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img src=\"https://docs.google.com/uc?export=download&id=1k7c1Xz4lcKqQeCWYIaKH_CD3e2MW210R\" alt=\"picture\" width=\"100%\">"
      ],
      "metadata": {
        "id": "Mj-yVWtTMdQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Normalización** </font>\n",
        "\n",
        "Esto es una técnica utilizada en el preprocesamiento de datos para modificar y ajustar los valores de las variables y escalarlas en el conjunto de datos común. En la normalización los valores se desplazan y reescalan para que terminen en un rango dado. Por ejemplo entre $[0, 1]$ o  $[-1, 1]$.\n",
        "\n",
        "La normalización es útil cuando no hay valores atípicos ya que no puede manejarlos.  Es posible usar distintos tipos de norma al escalar los datos y eso permite diferenciar entre diferentes operadores\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f9ZfpLuicy1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='157699'> **Min-Max scaling** </font>\n",
        "\n",
        "\n",
        "Es una técnica de escalado de datos en la que los puntos de datos se desplazan y reescalan para que terminen en un rango de 0 a 1. También se conoce como *min-max scaling*.\n",
        "\n",
        "La fórmula para calcular la puntuación normalizada:\n",
        "\n",
        "> $x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
        "\n",
        "Donde:\n",
        "\n",
        "* $x_{norm}$: es el valor normalizado.\n",
        "* $x$: es el valor original.\n",
        "* $x_{min}$: es el valor mínimo de la característica.\n",
        "* $x_{max}$: es el valor máximo de la característica.\n"
      ],
      "metadata": {
        "id": "yoUO3w9eeNLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.exponential(size=1000)\n",
        "# # get some negative values\n",
        "# for i in range(len(original_data)):\n",
        "#   if np.random.uniform() < 0.1:\n",
        "#     original_data[i]=-original_data[i]\n",
        "\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qpW6Oaaitniu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos esta función para que pueda usarse despues"
      ],
      "metadata": {
        "id": "BjegsdM9tshI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creamos esta función para que pueda usarse despues\n",
        "def compare_plot(df):\n",
        "  # Create subplots\n",
        "  fig = make_subplots(rows=1, cols=2, subplot_titles=['Sin escalar', 'Escalados'])\n",
        "\n",
        "  # Add traces to subplots\n",
        "  fig.add_trace(go.Histogram(x=df['original_data'], showlegend=False),row=1, col=1)\n",
        "  fig.add_trace(go.Histogram(x=df['scaled_data'], showlegend=False), row=1, col=2)\n",
        "\n",
        "  return fig"
      ],
      "metadata": {
        "id": "RFVd59lLtW7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = compare_plot(df)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "G2wFbEh8tY5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='157699'> **MaxAbs scaling** </font>\n",
        "\n",
        "Este estimador escala de manera que el valor absoluto máximo de cada característica en el conjunto de entrenamiento será 1.0. Es similar a `Min-Max`, excepto que los valores se asignan a varios rangos dependiendo de si están presentes valores negativos o positivos. Si solo están presentes valores positivos, el rango es `[0, 1]`. Si solo están presentes valores negativos, el rango es `[-1, 0]`. Si están presentes valores positivos y negativos, el rango es `[-1, 1]`. En datos solo positivos, tanto `Min-Max` como `Max-Abs` se comportan de manera similar.\n",
        "\n",
        "> $x_{\\text{scaled}} = \\frac{x}{\\max(|X|)}$\n",
        "\n",
        "donde:\n",
        "- $x$ es el valor original de la característica.\n",
        "- $\\max(|X|)$ es el valor absoluto máximo en la característica.\n",
        "- $x_{\\text{scaled}}$ es el valor escalado.\n",
        "\n",
        "Este método de escalado transforma los datos para que estén en el rango de $[-1, 1]$, manteniendo la proporción original entre los valores. Es útil cuando los datos tienen valores dispersos y se desea mantener la distribución original de los datos."
      ],
      "metadata": {
        "id": "A7zcoucPvhQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.exponential(size=1000)\n",
        "# get some negative values\n",
        "for i in range(len(original_data)):\n",
        "  if np.random.uniform() < 0.1:\n",
        "    original_data[i]=-original_data[i]\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MaxAbsScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "df"
      ],
      "metadata": {
        "id": "cWCsSHkzwITN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = compare_plot(df)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "Tssmz4bVwNLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Estandarización** </font>\n",
        "\n",
        "\n",
        "La estandarización se basa principalmente en las tendencias centrales y la varianza de los datos, transfora los datos para centrarlos eliminando el valor de una medida de tendencia central de cada característica y escala con base en una medida de dispersión."
      ],
      "metadata": {
        "id": "z5Ys1U2B09BG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='157699'> **Standar scaling** </font>"
      ],
      "metadata": {
        "id": "INYX5c_h1d3F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corresponde la estandarización de los datos de los datos alrededor de la media 0 y con una desviación estándar de 1. Es apropiada en aquellos casos en que la distribución de los datos es normal, aunque usualmente también se aplica cuando dicha distribucipon es desconocida.\n",
        "\n",
        "Claro, la fórmula de la estandarización de un dato \\( x \\) para una característica en particular se expresa en LaTeX de la siguiente manera:\n",
        "\n",
        "> $z = \\frac{x - \\mu}{\\sigma}$\n",
        "\n",
        "donde:\n",
        "- $x$ es el valor original de la característica.\n",
        "- $\\mu$ es la media de la característica.\n",
        "- $\\sigma$ es la desviación estándar de la característica.\n",
        "- $z$ es el valor estandarizado.\n",
        "\n",
        "Esto transforma el valor $x$ en un valor $z$ que tiene una media de $0$ y una desviación estándar de $1$."
      ],
      "metadata": {
        "id": "R9iJEjEM1axi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.exponential(size=1000)\n",
        "# # get some negative values\n",
        "# for i in range(len(original_data)):\n",
        "#   if np.random.uniform() < 0.1:\n",
        "#     original_data[i]=-original_data[i]\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = StandardScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "df"
      ],
      "metadata": {
        "id": "7daGzTEZ4S_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = compare_plot(df)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "RwYwRT2V4ch3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.normal(loc=6, scale=3, size=1000)\n",
        "# get some outliers\n",
        "for i in range(len(original_data)):\n",
        "  if np.random.uniform() < 0.02:\n",
        "    original_data[i]=10*original_data[i]\n",
        "\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = StandardScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "fig = compare_plot(df)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "ED0uGUhZ5Ftj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color='157699'> **Robust scaling** </font>\n",
        "\n",
        "En la presencia de valores atípicos, es probable que el escalado utilizando la media y la varianza de los datos no funcione muy bien. En estos casos, pueden usarse estimaciones más robustas para el centro y el rango de tus datos, como la mediana y el rango intercuartílico (IQR), que son menos sensibles a la influencia de los outliers.\n",
        "\n",
        "\n",
        "\n",
        "$x_{\\text{scaled}} = \\frac{x - \\text{median}(X)}{\\text{IQR}(X)}$\n",
        "\n",
        "donde:\n",
        "- $\\text{median}(X)$ es la mediana de la característica $X$.\n",
        "- $\\text{IQR}(X)$ es el rango intercuartílico de la característica $X $\n",
        "- $x$ es el valor original de la característica.\n",
        "- $x_{\\text{scaled}}$ es el valor escalado.\n",
        "\n",
        "El Robust Scaler es útil cuando se desea reducir el efecto de valores atípicos en el preprocesamiento de datos."
      ],
      "metadata": {
        "id": "hsNtA5S36Rcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.normal(loc=6, scale=3, size=1000)\n",
        "# get some outliers\n",
        "for i in range(len(original_data)):\n",
        "  if np.random.uniform() < 0.02:\n",
        "    original_data[i]=10*original_data[i]\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = RobustScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "fig = compare_plot(df)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "3KsKvJQd-A8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Reversar escalamiento** </font>\n",
        "\n",
        "En algunos casos puede ser necesario reversar el proceso de escalamiento aplicado a una variable, para ello puede ser de utilidad la función `scaler.inverse_transform`"
      ],
      "metadata": {
        "id": "dvXJzJme_L70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# generate 1000 data points randomly drawn from an exponential distribution\n",
        "original_data = np.random.exponential(size=1000)\n",
        "df = pd.DataFrame({'original_data': original_data})\n",
        "\n",
        "# Create a MinMaxScaler object\n",
        "scaler = MinMaxScaler()\n",
        "df['scaled_data'] = scaler.fit_transform(df[['original_data']])\n",
        "\n",
        "# reversa la operación\n",
        "df['reversed']= scaler.inverse_transform(df[['scaled_data']])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "-w0eDgkw_qYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Librerias para preprocesamiento** </font>\n",
        "Hemos usado las funciones para preprocesamiento de libreria `sklearn`, en particular los distintos tipos de `scalers` que ella ofrece. Sin embargo, es importante notar que existen diversas librerias que permiten funcionalidades similares tales como:\n",
        "\n",
        "* `Scikit-learn`\n",
        "* `Statsmodels`\n",
        "* `Scipy`\n",
        "* `Feature-engine`\n",
        "* `Pandas`\n",
        "\n"
      ],
      "metadata": {
        "id": "_eyUUHaGBGCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si bien, es recomendable y práctico usar las librerias y sus funciones, es importante notar que haciendo uso de los concpetos vistos hasta ahora de pandas, podriamos lograr estos mismos resultador. Por ejemplo, hacer la normalización `min-max` de una variable sin usar ninguna libreria adicional:\n"
      ],
      "metadata": {
        "id": "Waai_AG3Cu4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df['min_max'] = (df['original_data']-df['original_data'].min())/(df['original_data'].max()-df['original_data'].min())\n",
        "df.head()"
      ],
      "metadata": {
        "id": "5aVkRLjCC4d6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Pandas `pipeline`** </font>\n",
        "\n",
        "Los **pipelines** desempeñan un papel útil en la transformación y manipulación de grandes cantidades de datos. Un **pipeline** es una secuencia de mecanismos de procesamiento de datos. Estos nos permite encadenar varias funciones definidas por el usuario en Python para construir una secuencia de procesamiento de datos.\n",
        "\n"
      ],
      "metadata": {
        "id": "_hyAk-oFjv-H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así por ejemplo. Dado un conjunto de datos, podriamos estar interesados en automatizar la aplicación de las siguientes operaciones:\n",
        "> ```\n",
        "  Eliminar duplicados\n",
        "  Eliminar registro con datos null en ciertas columnas\n",
        "  Estandarizar ciertas columnas\n",
        "  ```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enHfbC8ukzjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para ellos sería de utilidad crear un pipeline. Veamos como funcionaria en el siguiente ejemplo:\n",
        "\n"
      ],
      "metadata": {
        "id": "oVZlcoj3lqyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='46B8A9'> **Ejemplo** </font>\n",
        "\n",
        "Considere el siguiente dataframe con los datos de registro de algunos usuarios, el tiempo activo en la plataforma, su genero, la experiencia en codificación y el ingreso mensual"
      ],
      "metadata": {
        "id": "3QvH-JhjjlHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "!gdown 16V6SHcpOeRMUewwlRZPIUtkaa3n4200d\n",
        "df = pd.read_csv('registros_v3.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "8XuDW4uUf183"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos las siguientes funciones:\n",
        "\n",
        "* `remove_duplicates()`: Elimina registros duplicados\n",
        "* `remove_nulls_columns()`: Elimina los registros que tengan valores nulos en las columnas `tiempo activo` e `ingreso`\n",
        "* `scale_columns()`: Normaliza o estandariza la columnas `ingresos` y `tiempo activo`. Note que esta función recibe como uno de sus argumentos el tipo la función del tipo de escalamiento que deseamos aplicar a las variables\n"
      ],
      "metadata": {
        "id": "1gOOZwD8mtl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicates(df):\n",
        "    return df.drop_duplicates()\n",
        "\n",
        "def remove_nulls_columns(df):\n",
        "    return df.dropna(subset=['tiempo_activo', 'ingreso'])\n",
        "\n",
        "def scale_columns(df, scaler):\n",
        "    scaler = scaler\n",
        "    df[['tiempo_activo', 'ingreso']] = scaler.fit_transform(df[['tiempo_activo', 'ingreso']])\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "iee4XrNGm0gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora enlazamos la funciones a través de un `pipe`"
      ],
      "metadata": {
        "id": "b9MyyRo1qGjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "df_process = (\n",
        "    df.pipe(remove_duplicates)\n",
        "    .pipe(remove_nulls_columns)\n",
        "    .pipe(scale_columns, scaler=StandardScaler())\n",
        ")\n",
        "df_process"
      ],
      "metadata": {
        "id": "zrMrpKGvpOpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color='46B8A9'> **Ejercicio** </font>\n",
        "\n",
        "Considere el ejemplo anterior, genere un pipeline que dado el df original, realice las tareas necesarias para obtener un gráfico de dispersión (scatter) de los ingreso contra el tiempo de actividad en la plataforma"
      ],
      "metadata": {
        "id": "0GDKUbLDqV7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Escriba aquí su respuesta\n"
      ],
      "metadata": {
        "id": "qfspOtpAq3nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color='056938'> **Reto de aplicación** </font>\n",
        "\n",
        "Para este ejercicio, utilizaremos el conjunto de datos *Mall Customers* disponible en [Kaggle](https://www.kaggle.com/datasets/shwetabh123/mall-customers). Es un conjunto de datos  basado en un escenario hipotético. Contiene información básica sobre $200$ clientes ficticios de un centro comercial. El conjunto de datos consta de 5 columnas:\n",
        "\n",
        "- `CustomerID`: Un identificador único para cada cliente.\n",
        "- `Gender`: El género del cliente.\n",
        "- `Age`: La edad del cliente.\n",
        "- `Annual Income`: El ingreso anual del cliente (en miles de dólares).\n",
        "- `Spending Score`: Una puntuación asignada al cliente en función de sus hábitos de gasto. La puntuación varía de 1 a 100, donde una puntuación más alta indica un cliente que gasta más.\n",
        "- `Genre_Binary`: la columna `Gender` convertida a binario"
      ],
      "metadata": {
        "id": "zQauJkwQq6o9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestro interes es segmentar los clientes en `k `grupos de clientes similares de modo que podemos hacer campañas de mercadeo específicas para cada grupo"
      ],
      "metadata": {
        "id": "ukktQ_63xFCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load data\n",
        "!gdown 1LELVYMHNjY7H97asN_QUtybd4UWfcG-U\n",
        "\n",
        "# create dataframe\n",
        "df = pd.read_csv('mall_customers.csv')\n",
        "df = df.rename(columns={\n",
        "    'Annual Income (k$)': 'Annual Income',\n",
        "    'Spending Score (1-100)': 'Spending Score'\n",
        "})\n",
        "# Mapping Gender to binary values: M -> 1, F -> 0\n",
        "df['Genre_Binary'] = df['Genre'].map({'Female': 1, 'Male': 0})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "o34nidXxtHur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La siguiente función agrupa los clientes en `k` grupos, usando el algoritmo `k-means`, con base en las columnas `['Age', 'Annual Income',\t'Spending Score',\t'Genre_Binary']`"
      ],
      "metadata": {
        "id": "W7O5q_ODwB3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "def k_means(df, k, columns):\n",
        "  # Fit the model to the unscaled data\n",
        "  df_kmeans = df[columns]\n",
        "  kmeans = KMeans(n_clusters=k, n_init= 10,random_state=42)\n",
        "  kmeans.fit(df_kmeans)\n",
        "  labels = kmeans.labels_\n",
        "  df['Cluster'] = labels\n",
        "\n",
        "  return df\n",
        "\n",
        "# aplicamos la función de agrupación\n",
        "k = 5\n",
        "columns =  ['Age', 'Annual Income',\t'Spending Score',\t'Genre_Binary']\n",
        "df_clustered = k_means(df, k, columns)\n",
        "df_clustered.head()"
      ],
      "metadata": {
        "id": "-POs0u_1mEeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por su parte la siguiente función permite graficar las agrupaciones obtenidas con respecto a distintos pares de variables:"
      ],
      "metadata": {
        "id": "Ng5E5nKMyuHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots\n",
        "def plot_clusters(df_clustered):\n",
        "  fig = make_subplots(rows=1, cols=3, subplot_titles=['Age vs Annual Income', 'Age vs Spending Score', 'Annual Income vs Spending Score'])\n",
        "\n",
        "  # Add traces to subplots\n",
        "  fig.add_trace(go.Scatter(x=df_clustered['Age'], y=df['Annual Income'],\n",
        "                          mode='markers', marker_color=df['Cluster'], marker_size=10, showlegend=False), row=1, col=1)\n",
        "  fig.add_trace(go.Scatter(x=df_clustered['Age'], y=df['Spending Score'],\n",
        "                          mode='markers', marker_color=df['Cluster'], marker_size=10, showlegend=False), row=1, col=2)\n",
        "  fig.add_trace(go.Scatter(x=df_clustered['Annual Income'], y=df['Spending Score'],\n",
        "                          mode='markers', marker_color=df['Cluster'], marker_size=10, showlegend=False), row=1, col=3)\n",
        "\n",
        "  return fig\n",
        "\n",
        "fig = plot_clusters(df_clustered)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "h4A4IhCtxwcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realice las operaciones necesarias que considere, escalando las vaiables, y compare los resultados que obtiene en las agrpaciones. Use las funciones definidas (`k_means()` y `plot_clusters())` para agrupar y graficar"
      ],
      "metadata": {
        "id": "CqxQZs1l1IJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# escriba aquí su respuesta"
      ],
      "metadata": {
        "id": "CDJDd2Qg1h-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}